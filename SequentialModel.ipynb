{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9ac3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 18:03:52.088695: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6add72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Status</th>\n",
       "      <th>Life expectancy</th>\n",
       "      <th>Adult Mortality</th>\n",
       "      <th>infant deaths</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>percentage expenditure</th>\n",
       "      <th>Hepatitis B</th>\n",
       "      <th>Measles</th>\n",
       "      <th>...</th>\n",
       "      <th>Polio</th>\n",
       "      <th>Total expenditure</th>\n",
       "      <th>Diphtheria</th>\n",
       "      <th>HIV/AIDS</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Population</th>\n",
       "      <th>thinness  1-19 years</th>\n",
       "      <th>thinness 5-9 years</th>\n",
       "      <th>Income composition of resources</th>\n",
       "      <th>Schooling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>2003</td>\n",
       "      <td>Developing</td>\n",
       "      <td>71.6</td>\n",
       "      <td>135.0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.42</td>\n",
       "      <td>459.111306</td>\n",
       "      <td>68.0</td>\n",
       "      <td>5844</td>\n",
       "      <td>...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>5.34</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4718.512910</td>\n",
       "      <td>668583.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.668</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>Solomon Islands</td>\n",
       "      <td>2012</td>\n",
       "      <td>Developing</td>\n",
       "      <td>68.7</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>247.949228</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>5.48</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1858.689868</td>\n",
       "      <td>551531.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.505</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>South Sudan</td>\n",
       "      <td>2005</td>\n",
       "      <td>Developing</td>\n",
       "      <td>51.9</td>\n",
       "      <td>383.0</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>818877.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>Maldives</td>\n",
       "      <td>2004</td>\n",
       "      <td>Developing</td>\n",
       "      <td>73.4</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>611.909357</td>\n",
       "      <td>97.0</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>5.89</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3853.333480</td>\n",
       "      <td>312.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>14.6</td>\n",
       "      <td>0.617</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2682</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>2014</td>\n",
       "      <td>Developing</td>\n",
       "      <td>75.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1.45</td>\n",
       "      <td>181.908378</td>\n",
       "      <td>96.0</td>\n",
       "      <td>565</td>\n",
       "      <td>...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>5.41</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>12127.225220</td>\n",
       "      <td>773628.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.759</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Country  Year      Status  Life expectancy   Adult Mortality  \\\n",
       "2693           Turkey  2003  Developing              71.6            135.0   \n",
       "2364  Solomon Islands  2012  Developing              68.7            184.0   \n",
       "2419      South Sudan  2005  Developing              51.9            383.0   \n",
       "1613         Maldives  2004  Developing              73.4             16.0   \n",
       "2682           Turkey  2014  Developing              75.5             17.0   \n",
       "\n",
       "      infant deaths  Alcohol  percentage expenditure  Hepatitis B  Measles   \\\n",
       "2693             35     1.42              459.111306         68.0      5844   \n",
       "2364              0     0.01              247.949228         99.0         0   \n",
       "2419             28      NaN                0.000000          NaN         0   \n",
       "1613              0     1.60              611.909357         97.0        37   \n",
       "2682             16     1.45              181.908378         96.0       565   \n",
       "\n",
       "      ...  Polio  Total expenditure  Diphtheria    HIV/AIDS           GDP  \\\n",
       "2693  ...   69.0               5.34         68.0        0.1   4718.512910   \n",
       "2364  ...   99.0               5.48         99.0        0.1   1858.689868   \n",
       "2419  ...    NaN                NaN          NaN        3.9           NaN   \n",
       "1613  ...   96.0               5.89         96.0        0.1   3853.333480   \n",
       "2682  ...   96.0               5.41         96.0        0.1  12127.225220   \n",
       "\n",
       "      Population   thinness  1-19 years   thinness 5-9 years  \\\n",
       "2693    668583.0                    5.1                  5.0   \n",
       "2364    551531.0                    1.2                  1.2   \n",
       "2419    818877.0                    NaN                  NaN   \n",
       "1613       312.0                   14.5                 14.6   \n",
       "2682    773628.0                    4.9                  4.7   \n",
       "\n",
       "      Income composition of resources  Schooling  \n",
       "2693                            0.668       11.9  \n",
       "2364                            0.505        9.4  \n",
       "2419                            0.000        0.0  \n",
       "1613                            0.617       12.2  \n",
       "2682                            0.759       14.5  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../datasets/life_expectancy.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e84d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                              0\n",
       "Year                                 0\n",
       "Status                               0\n",
       "Life expectancy                     10\n",
       "Adult Mortality                     10\n",
       "infant deaths                        0\n",
       "Alcohol                            194\n",
       "percentage expenditure               0\n",
       "Hepatitis B                        553\n",
       "Measles                              0\n",
       " BMI                                34\n",
       "under-five deaths                    0\n",
       "Polio                               19\n",
       "Total expenditure                  226\n",
       "Diphtheria                          19\n",
       " HIV/AIDS                            0\n",
       "GDP                                448\n",
       "Population                         652\n",
       " thinness  1-19 years               34\n",
       " thinness 5-9 years                 34\n",
       "Income composition of resources    167\n",
       "Schooling                          163\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum() #find columns with missing fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28dcb7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = data['Country'].unique()\n",
    "na_cols = ['Life expectancy ', 'Adult Mortality', 'Alcohol', 'Hepatitis B', ' BMI ', 'Polio',\n",
    "           'Total expenditure', 'Diphtheria ', 'GDP', ' thinness  1-19 years', ' thinness 5-9 years',\n",
    "           'Population', 'Income composition of resources']\n",
    "for col in na_cols:\n",
    "    for country in countries:\n",
    "        data.loc[data['Country']== country, col] = data.loc[data['Country']== country, col]\\\n",
    "                                                        .fillna(data[data['Country'] == country][col].mean())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a7631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum() # still missing data\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc80f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status\n",
       "Developing    1824\n",
       "Developed      304\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff25d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "data.boxplot('Life expectancy ') # box plot of life expectancy across countries\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0439694a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "boxplot() got multiple values for argument 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# compare between deveeloped and developing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboxplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStatus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLife expectancy \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal expenditure\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: boxplot() got multiple values for argument 'data'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare between deveeloped and developing\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot('Status', 'Life expectancy ', data=data)\n",
    "plt.xlabel('Status', fontsize=16)\n",
    "plt.ylabel('Total expenditure', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e628755",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop('Life expectancy ', axis=1) # features from the data\n",
    "target = data[['Life expectancy ']]\n",
    "features = features.drop('Country', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "784de330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Developing\n",
       "1    Developing\n",
       "2    Developing\n",
       "3    Developing\n",
       "4    Developing\n",
       "Name: Status, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# everything is numerical except for Status, putting in separate dataframe\n",
    "# to do one-hot encoding\n",
    "categorical_features = features['Status'].copy()\n",
    "categorical_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dff1e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Developed</th>\n",
       "      <th>Developing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Developed  Developing\n",
       "0      False        True\n",
       "1      False        True\n",
       "2      False        True\n",
       "3      False        True\n",
       "4      False        True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = pd.get_dummies(categorical_features)\n",
    "categorical_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d091bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = features.drop(['Status'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eca61d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-1.626978</td>\n",
       "      <td>-0.813489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.813489</td>\n",
       "      <td>1.626978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adult Mortality</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>-5.342427e-17</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-1.335866</td>\n",
       "      <td>-0.757737</td>\n",
       "      <td>-0.171899</td>\n",
       "      <td>0.516075</td>\n",
       "      <td>4.229591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infant deaths</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-0.263617</td>\n",
       "      <td>-0.256275</td>\n",
       "      <td>-0.234247</td>\n",
       "      <td>-0.087396</td>\n",
       "      <td>12.952948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alcohol</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>1.402387e-16</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-1.117358</td>\n",
       "      <td>-0.911020</td>\n",
       "      <td>-0.177159</td>\n",
       "      <td>0.687950</td>\n",
       "      <td>3.390549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage expenditure</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>1.335607e-17</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-0.374224</td>\n",
       "      <td>-0.362599</td>\n",
       "      <td>-0.325301</td>\n",
       "      <td>-0.122022</td>\n",
       "      <td>10.711711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hepatitis B</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>3.005115e-17</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-2.980588</td>\n",
       "      <td>-0.367151</td>\n",
       "      <td>0.385202</td>\n",
       "      <td>0.701983</td>\n",
       "      <td>0.860373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Measles</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-0.223140</td>\n",
       "      <td>-0.223140</td>\n",
       "      <td>-0.221460</td>\n",
       "      <td>-0.184578</td>\n",
       "      <td>17.593236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>4.674623e-17</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-1.778912</td>\n",
       "      <td>-0.922017</td>\n",
       "      <td>0.108777</td>\n",
       "      <td>0.932909</td>\n",
       "      <td>2.061994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>under-five deaths</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>-1.669508e-17</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-0.269123</td>\n",
       "      <td>-0.263721</td>\n",
       "      <td>-0.247514</td>\n",
       "      <td>-0.084097</td>\n",
       "      <td>13.236418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polio</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>8.347542e-17</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-3.210181</td>\n",
       "      <td>-0.230588</td>\n",
       "      <td>0.431544</td>\n",
       "      <td>0.638460</td>\n",
       "      <td>0.762610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total expenditure</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>-9.683148e-17</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-2.446505</td>\n",
       "      <td>-0.674273</td>\n",
       "      <td>-0.039196</td>\n",
       "      <td>0.640437</td>\n",
       "      <td>3.769052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diphtheria</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>7.679738e-17</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-3.256291</td>\n",
       "      <td>-0.194464</td>\n",
       "      <td>0.426177</td>\n",
       "      <td>0.633057</td>\n",
       "      <td>0.757185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HIV/AIDS</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>-1.335607e-17</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-0.355934</td>\n",
       "      <td>-0.355934</td>\n",
       "      <td>-0.355934</td>\n",
       "      <td>-0.149957</td>\n",
       "      <td>8.312249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GDP</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>-5.342427e-17</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-0.475481</td>\n",
       "      <td>-0.440813</td>\n",
       "      <td>-0.356808</td>\n",
       "      <td>-0.080407</td>\n",
       "      <td>10.004268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Population</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>-4.340722e-17</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-0.212229</td>\n",
       "      <td>-0.209079</td>\n",
       "      <td>-0.189524</td>\n",
       "      <td>-0.088929</td>\n",
       "      <td>20.278937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thinness  1-19 years</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>1.402387e-16</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-1.063057</td>\n",
       "      <td>-0.744642</td>\n",
       "      <td>-0.362545</td>\n",
       "      <td>0.513094</td>\n",
       "      <td>4.795766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thinness 5-9 years</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>1.202046e-16</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-1.056885</td>\n",
       "      <td>-0.744661</td>\n",
       "      <td>-0.369993</td>\n",
       "      <td>0.504233</td>\n",
       "      <td>4.875365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Income composition of resources</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>-6.344132e-17</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-2.987278</td>\n",
       "      <td>-0.642867</td>\n",
       "      <td>0.206629</td>\n",
       "      <td>0.686084</td>\n",
       "      <td>1.620407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schooling</th>\n",
       "      <td>2128.0</td>\n",
       "      <td>-3.138676e-16</td>\n",
       "      <td>1.000235</td>\n",
       "      <td>-3.650831</td>\n",
       "      <td>-0.590731</td>\n",
       "      <td>0.065004</td>\n",
       "      <td>0.658289</td>\n",
       "      <td>2.812849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  count          mean       std       min  \\\n",
       "Year                             2128.0  0.000000e+00  1.000235 -1.626978   \n",
       "Adult Mortality                  2128.0 -5.342427e-17  1.000235 -1.335866   \n",
       "infant deaths                    2128.0  0.000000e+00  1.000235 -0.263617   \n",
       "Alcohol                          2128.0  1.402387e-16  1.000235 -1.117358   \n",
       "percentage expenditure           2128.0  1.335607e-17  1.000235 -0.374224   \n",
       "Hepatitis B                      2128.0  3.005115e-17  1.000235 -2.980588   \n",
       "Measles                          2128.0  0.000000e+00  1.000235 -0.223140   \n",
       " BMI                             2128.0  4.674623e-17  1.000235 -1.778912   \n",
       "under-five deaths                2128.0 -1.669508e-17  1.000235 -0.269123   \n",
       "Polio                            2128.0  8.347542e-17  1.000235 -3.210181   \n",
       "Total expenditure                2128.0 -9.683148e-17  1.000235 -2.446505   \n",
       "Diphtheria                       2128.0  7.679738e-17  1.000235 -3.256291   \n",
       " HIV/AIDS                        2128.0 -1.335607e-17  1.000235 -0.355934   \n",
       "GDP                              2128.0 -5.342427e-17  1.000235 -0.475481   \n",
       "Population                       2128.0 -4.340722e-17  1.000235 -0.212229   \n",
       " thinness  1-19 years            2128.0  1.402387e-16  1.000235 -1.063057   \n",
       " thinness 5-9 years              2128.0  1.202046e-16  1.000235 -1.056885   \n",
       "Income composition of resources  2128.0 -6.344132e-17  1.000235 -2.987278   \n",
       "Schooling                        2128.0 -3.138676e-16  1.000235 -3.650831   \n",
       "\n",
       "                                      25%       50%       75%        max  \n",
       "Year                            -0.813489  0.000000  0.813489   1.626978  \n",
       "Adult Mortality                 -0.757737 -0.171899  0.516075   4.229591  \n",
       "infant deaths                   -0.256275 -0.234247 -0.087396  12.952948  \n",
       "Alcohol                         -0.911020 -0.177159  0.687950   3.390549  \n",
       "percentage expenditure          -0.362599 -0.325301 -0.122022  10.711711  \n",
       "Hepatitis B                     -0.367151  0.385202  0.701983   0.860373  \n",
       "Measles                         -0.223140 -0.221460 -0.184578  17.593236  \n",
       " BMI                            -0.922017  0.108777  0.932909   2.061994  \n",
       "under-five deaths               -0.263721 -0.247514 -0.084097  13.236418  \n",
       "Polio                           -0.230588  0.431544  0.638460   0.762610  \n",
       "Total expenditure               -0.674273 -0.039196  0.640437   3.769052  \n",
       "Diphtheria                      -0.194464  0.426177  0.633057   0.757185  \n",
       " HIV/AIDS                       -0.355934 -0.355934 -0.149957   8.312249  \n",
       "GDP                             -0.440813 -0.356808 -0.080407  10.004268  \n",
       "Population                      -0.209079 -0.189524 -0.088929  20.278937  \n",
       " thinness  1-19 years           -0.744642 -0.362545  0.513094   4.795766  \n",
       " thinness 5-9 years             -0.744661 -0.369993  0.504233   4.875365  \n",
       "Income composition of resources -0.642867  0.206629  0.686084   1.620407  \n",
       "Schooling                       -0.590731  0.065004  0.658289   2.812849  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#standardizaton of data to Z-scores\n",
    "standardScaler = StandardScaler()\n",
    "numeric_features = pd.DataFrame(standardScaler.fit_transform(numeric_features),\n",
    "                               columns=numeric_features.columns,\n",
    "                               index=numeric_features.index)\n",
    "numeric_features.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e03b075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 21)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_features = pd.concat([numeric_features, categorical_features], axis=1,\n",
    "                              sort=False)\n",
    "processed_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e30de6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 21), (426, 21)), ((1702, 1), (426, 1)))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(processed_features, target,\n",
    "                                                   test_size=0.2, random_state=1)\n",
    "(x_train.shape, x_test.shape), (y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67602d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_single_layer_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    # first layer is dense layer with 32 neurons\n",
    "    #number of columns in x is second element in x shape\n",
    "    model.add(tf.keras.layers.Dense(32,\n",
    "                                   input_shape = (x_train.shape[1],),\n",
    "                                   activation = 'sigmoid'))\n",
    "    #output/prediction layer, only one output (life expectancy)\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "    # popular optimizer = Adam\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(loss = 'mse',\n",
    "                 metrics = ['mae', 'mse'],\n",
    "                 optimizer = optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34be1859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                704       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 737 (2.88 KB)\n",
      "Trainable params: 737 (2.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_single_layer_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilize model\n",
    "!pip install pydot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28eb2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "#import graphviz\n",
    "#tf.keras.utils.plot_model(model) # need to install some additional packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb226571",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# shows how model parameters change over the cours eof training\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "# shows how model parameters change over the cours eof training\n",
    "training_history = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            epochs = num_epochs,\n",
    "                            validation_split = 0.2,\n",
    "                            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331812f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(training_history.history['mae'])\n",
    "plt.plot(training_history.history['val_mae'])\n",
    "\n",
    "plt.title('Model MAE')\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val']) # training and validation data\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(training_history.history['loss'])\n",
    "plt.plot(training_history.history['val_loss'])\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "r2_score(y_test, y_pred) # higher the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = pd.DataFrame({'y_test': y_test.values.flatten(),\n",
    "                            'y_pred': y_pred.flatten()}, \n",
    "                            index = range(len(y_pred)))\n",
    "\n",
    "pred_results.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6fb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(y_test, y_pred, s=100, c='blue')\n",
    "plt.xlabel('Actual life expectancy')\n",
    "plt.ylabel('Predicted life expectancy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b351bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multiple_layer_model():\n",
    "    #relu activation function\n",
    "    model = keras.Sequential([layers.Dense(32, input_shape = (x_train.shape[1],), activation='relu'),\n",
    "                              layers.Dense(16, activation = 'relu'),\n",
    "                              layers.Dense(4, activation = 'relu'),\n",
    "                              layers.Dense(1)])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    \n",
    "    model.compile(loss = 'mse', metrics = ['mae', 'mse'], optimizer = optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f865fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_multiple_layer_model()\n",
    "tf.keras.utils.plot_model(model, show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf seq_logs\n",
    "!lz -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb237433",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"seq_logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91318c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 20.5736 - mae: 3.4971 - mse: 20.5736 - val_loss: 26.1270 - val_mae: 3.8863 - val_mse: 26.1270\n",
      "Epoch 99/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 20.3255 - mae: 3.4775 - mse: 20.3255 - val_loss: 25.9476 - val_mae: 3.8749 - val_mse: 25.9476\n",
      "Epoch 100/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 20.0244 - mae: 3.4560 - mse: 20.0244 - val_loss: 25.4053 - val_mae: 3.8249 - val_mse: 25.4053\n",
      "Epoch 101/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 19.7854 - mae: 3.4293 - mse: 19.7854 - val_loss: 25.3769 - val_mae: 3.8125 - val_mse: 25.3769\n",
      "Epoch 102/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.5493 - mae: 3.4127 - mse: 19.5493 - val_loss: 25.0157 - val_mae: 3.7922 - val_mse: 25.0157\n",
      "Epoch 103/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.3119 - mae: 3.3925 - mse: 19.3119 - val_loss: 24.4541 - val_mae: 3.7491 - val_mse: 24.4541\n",
      "Epoch 104/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 19.0590 - mae: 3.3732 - mse: 19.0590 - val_loss: 24.4319 - val_mae: 3.7438 - val_mse: 24.4319\n",
      "Epoch 105/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 18.6797 - mae: 3.3349 - mse: 18.6797 - val_loss: 23.9766 - val_mae: 3.7008 - val_mse: 23.9766\n",
      "Epoch 106/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 18.4627 - mae: 3.3165 - mse: 18.4627 - val_loss: 23.7980 - val_mae: 3.6834 - val_mse: 23.7980\n",
      "Epoch 107/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 18.2270 - mae: 3.3009 - mse: 18.2270 - val_loss: 23.3340 - val_mae: 3.6403 - val_mse: 23.3340\n",
      "Epoch 108/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 17.9825 - mae: 3.2734 - mse: 17.9825 - val_loss: 23.1386 - val_mae: 3.6247 - val_mse: 23.1386\n",
      "Epoch 109/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.7625 - mae: 3.2607 - mse: 17.7625 - val_loss: 22.9010 - val_mae: 3.6156 - val_mse: 22.9010\n",
      "Epoch 110/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 17.6337 - mae: 3.2417 - mse: 17.6337 - val_loss: 22.5653 - val_mae: 3.5809 - val_mse: 22.5653\n",
      "Epoch 111/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 17.3919 - mae: 3.2322 - mse: 17.3919 - val_loss: 22.5114 - val_mae: 3.5749 - val_mse: 22.5114\n",
      "Epoch 112/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 17.0055 - mae: 3.1875 - mse: 17.0055 - val_loss: 22.0839 - val_mae: 3.5316 - val_mse: 22.0839\n",
      "Epoch 113/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 16.8768 - mae: 3.1787 - mse: 16.8768 - val_loss: 21.8600 - val_mae: 3.5171 - val_mse: 21.8600\n",
      "Epoch 114/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.6738 - mae: 3.1588 - mse: 16.6738 - val_loss: 21.5964 - val_mae: 3.4921 - val_mse: 21.5964\n",
      "Epoch 115/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.4431 - mae: 3.1360 - mse: 16.4431 - val_loss: 21.4179 - val_mae: 3.4700 - val_mse: 21.4179\n",
      "Epoch 116/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 16.2732 - mae: 3.1213 - mse: 16.2732 - val_loss: 21.1054 - val_mae: 3.4454 - val_mse: 21.1054\n",
      "Epoch 117/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 16.0550 - mae: 3.0986 - mse: 16.0550 - val_loss: 20.9162 - val_mae: 3.4220 - val_mse: 20.9162\n",
      "Epoch 118/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 15.8564 - mae: 3.0743 - mse: 15.8564 - val_loss: 20.7133 - val_mae: 3.4094 - val_mse: 20.7133\n",
      "Epoch 119/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 15.7146 - mae: 3.0694 - mse: 15.7146 - val_loss: 20.4888 - val_mae: 3.3901 - val_mse: 20.4888\n",
      "Epoch 120/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 15.5190 - mae: 3.0434 - mse: 15.5190 - val_loss: 20.2670 - val_mae: 3.3535 - val_mse: 20.2670\n",
      "Epoch 121/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 15.3151 - mae: 3.0258 - mse: 15.3151 - val_loss: 20.0468 - val_mae: 3.3597 - val_mse: 20.0468\n",
      "Epoch 122/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 15.1257 - mae: 3.0054 - mse: 15.1257 - val_loss: 19.9569 - val_mae: 3.3292 - val_mse: 19.9569\n",
      "Epoch 123/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.9899 - mae: 2.9898 - mse: 14.9899 - val_loss: 19.5708 - val_mae: 3.3089 - val_mse: 19.5708\n",
      "Epoch 124/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.8644 - mae: 2.9781 - mse: 14.8644 - val_loss: 19.5219 - val_mae: 3.2881 - val_mse: 19.5219\n",
      "Epoch 125/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.6772 - mae: 2.9613 - mse: 14.6772 - val_loss: 19.4640 - val_mae: 3.2887 - val_mse: 19.4640\n",
      "Epoch 126/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 14.4945 - mae: 2.9399 - mse: 14.4945 - val_loss: 19.1412 - val_mae: 3.2536 - val_mse: 19.1412\n",
      "Epoch 127/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.3047 - mae: 2.9184 - mse: 14.3047 - val_loss: 18.9522 - val_mae: 3.2349 - val_mse: 18.9522\n",
      "Epoch 128/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.2039 - mae: 2.9061 - mse: 14.2039 - val_loss: 18.7722 - val_mae: 3.2181 - val_mse: 18.7722\n",
      "Epoch 129/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 14.0488 - mae: 2.8990 - mse: 14.0488 - val_loss: 18.7464 - val_mae: 3.2243 - val_mse: 18.7464\n",
      "Epoch 130/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 13.9329 - mae: 2.8733 - mse: 13.9329 - val_loss: 18.4909 - val_mae: 3.1931 - val_mse: 18.4909\n",
      "Epoch 131/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.8323 - mae: 2.8706 - mse: 13.8323 - val_loss: 18.3541 - val_mae: 3.1616 - val_mse: 18.3541\n",
      "Epoch 132/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.6545 - mae: 2.8404 - mse: 13.6545 - val_loss: 18.1355 - val_mae: 3.1418 - val_mse: 18.1355\n",
      "Epoch 133/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.4796 - mae: 2.8321 - mse: 13.4796 - val_loss: 18.0282 - val_mae: 3.1590 - val_mse: 18.0282\n",
      "Epoch 134/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 13.3628 - mae: 2.8106 - mse: 13.3628 - val_loss: 17.8679 - val_mae: 3.1336 - val_mse: 17.8679\n",
      "Epoch 135/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 13.2298 - mae: 2.8019 - mse: 13.2298 - val_loss: 17.7664 - val_mae: 3.1090 - val_mse: 17.7664\n",
      "Epoch 136/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 13.0706 - mae: 2.7791 - mse: 13.0706 - val_loss: 17.5929 - val_mae: 3.0967 - val_mse: 17.5929\n",
      "Epoch 137/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 12.9236 - mae: 2.7664 - mse: 12.9236 - val_loss: 17.3688 - val_mae: 3.0665 - val_mse: 17.3688\n",
      "Epoch 138/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 12.8080 - mae: 2.7488 - mse: 12.8080 - val_loss: 17.2906 - val_mae: 3.0643 - val_mse: 17.2906\n",
      "Epoch 139/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 12.6838 - mae: 2.7359 - mse: 12.6838 - val_loss: 17.1559 - val_mae: 3.0432 - val_mse: 17.1559\n",
      "Epoch 140/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 12.6157 - mae: 2.7272 - mse: 12.6157 - val_loss: 16.9764 - val_mae: 3.0251 - val_mse: 16.9764\n",
      "Epoch 141/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 12.5136 - mae: 2.7210 - mse: 12.5136 - val_loss: 16.8561 - val_mae: 3.0130 - val_mse: 16.8561\n",
      "Epoch 142/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 12.3652 - mae: 2.6990 - mse: 12.3652 - val_loss: 16.7478 - val_mae: 2.9977 - val_mse: 16.7478\n",
      "Epoch 143/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 12.3084 - mae: 2.7015 - mse: 12.3084 - val_loss: 16.6911 - val_mae: 2.9941 - val_mse: 16.6911\n",
      "Epoch 144/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 12.1650 - mae: 2.6745 - mse: 12.1650 - val_loss: 16.4437 - val_mae: 2.9744 - val_mse: 16.4437\n",
      "Epoch 145/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 12.0784 - mae: 2.6725 - mse: 12.0784 - val_loss: 16.3010 - val_mae: 2.9645 - val_mse: 16.3010\n",
      "Epoch 146/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 6ms/step - loss: 12.0550 - mae: 2.6640 - mse: 12.0550 - val_loss: 16.2543 - val_mae: 2.9535 - val_mse: 16.2543\n",
      "Epoch 147/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 11.8770 - mae: 2.6480 - mse: 11.8770 - val_loss: 16.0355 - val_mae: 2.9248 - val_mse: 16.0355\n",
      "Epoch 148/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.7929 - mae: 2.6421 - mse: 11.7929 - val_loss: 15.9715 - val_mae: 2.9185 - val_mse: 15.9715\n",
      "Epoch 149/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.7089 - mae: 2.6154 - mse: 11.7089 - val_loss: 15.7664 - val_mae: 2.8987 - val_mse: 15.7664\n",
      "Epoch 150/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.6388 - mae: 2.6238 - mse: 11.6388 - val_loss: 15.8072 - val_mae: 2.9011 - val_mse: 15.8072\n",
      "Epoch 151/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.4997 - mae: 2.5991 - mse: 11.4997 - val_loss: 15.6927 - val_mae: 2.8832 - val_mse: 15.6927\n",
      "Epoch 152/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 11.4633 - mae: 2.5966 - mse: 11.4633 - val_loss: 15.5117 - val_mae: 2.8808 - val_mse: 15.5117\n",
      "Epoch 153/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.4706 - mae: 2.6009 - mse: 11.4706 - val_loss: 15.4519 - val_mae: 2.8616 - val_mse: 15.4519\n",
      "Epoch 154/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.2828 - mae: 2.5707 - mse: 11.2828 - val_loss: 15.2791 - val_mae: 2.8451 - val_mse: 15.2791\n",
      "Epoch 155/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.1797 - mae: 2.5668 - mse: 11.1797 - val_loss: 15.1295 - val_mae: 2.8223 - val_mse: 15.1295\n",
      "Epoch 156/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.0926 - mae: 2.5546 - mse: 11.0926 - val_loss: 15.1174 - val_mae: 2.8304 - val_mse: 15.1174\n",
      "Epoch 157/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 11.0104 - mae: 2.5463 - mse: 11.0104 - val_loss: 14.9649 - val_mae: 2.8108 - val_mse: 14.9649\n",
      "Epoch 158/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.9260 - mae: 2.5359 - mse: 10.9260 - val_loss: 14.8780 - val_mae: 2.8198 - val_mse: 14.8780\n",
      "Epoch 159/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.8494 - mae: 2.5294 - mse: 10.8494 - val_loss: 14.8475 - val_mae: 2.7952 - val_mse: 14.8475\n",
      "Epoch 160/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.7569 - mae: 2.5138 - mse: 10.7569 - val_loss: 14.7319 - val_mae: 2.7930 - val_mse: 14.7319\n",
      "Epoch 161/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.7143 - mae: 2.5077 - mse: 10.7143 - val_loss: 14.6296 - val_mae: 2.7647 - val_mse: 14.6296\n",
      "Epoch 162/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.6851 - mae: 2.5056 - mse: 10.6851 - val_loss: 14.6311 - val_mae: 2.7792 - val_mse: 14.6311\n",
      "Epoch 163/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.5666 - mae: 2.4911 - mse: 10.5666 - val_loss: 14.4714 - val_mae: 2.7559 - val_mse: 14.4714\n",
      "Epoch 164/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.4696 - mae: 2.4785 - mse: 10.4696 - val_loss: 14.3021 - val_mae: 2.7530 - val_mse: 14.3021\n",
      "Epoch 165/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 10.4086 - mae: 2.4717 - mse: 10.4086 - val_loss: 14.2432 - val_mae: 2.7424 - val_mse: 14.2432\n",
      "Epoch 166/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.3490 - mae: 2.4640 - mse: 10.3490 - val_loss: 14.1327 - val_mae: 2.7218 - val_mse: 14.1327\n",
      "Epoch 167/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 10.2932 - mae: 2.4609 - mse: 10.2932 - val_loss: 14.1013 - val_mae: 2.7326 - val_mse: 14.1013\n",
      "Epoch 168/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.2084 - mae: 2.4487 - mse: 10.2084 - val_loss: 14.0405 - val_mae: 2.7199 - val_mse: 14.0405\n",
      "Epoch 169/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.1710 - mae: 2.4441 - mse: 10.1710 - val_loss: 13.8064 - val_mae: 2.6877 - val_mse: 13.8064\n",
      "Epoch 170/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.0964 - mae: 2.4347 - mse: 10.0964 - val_loss: 13.9065 - val_mae: 2.7036 - val_mse: 13.9065\n",
      "Epoch 171/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 10.0710 - mae: 2.4375 - mse: 10.0710 - val_loss: 13.8100 - val_mae: 2.7047 - val_mse: 13.8100\n",
      "Epoch 172/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.9720 - mae: 2.4175 - mse: 9.9720 - val_loss: 13.6189 - val_mae: 2.6804 - val_mse: 13.6189\n",
      "Epoch 173/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.9085 - mae: 2.4061 - mse: 9.9085 - val_loss: 13.6311 - val_mae: 2.6662 - val_mse: 13.6311\n",
      "Epoch 174/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.8614 - mae: 2.4089 - mse: 9.8614 - val_loss: 13.5694 - val_mae: 2.6650 - val_mse: 13.5694\n",
      "Epoch 175/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.8922 - mae: 2.4047 - mse: 9.8922 - val_loss: 13.4571 - val_mae: 2.6641 - val_mse: 13.4571\n",
      "Epoch 176/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.7262 - mae: 2.3908 - mse: 9.7262 - val_loss: 13.3773 - val_mae: 2.6554 - val_mse: 13.3773\n",
      "Epoch 177/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.6991 - mae: 2.3890 - mse: 9.6991 - val_loss: 13.3923 - val_mae: 2.6513 - val_mse: 13.3923\n",
      "Epoch 178/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.6933 - mae: 2.3873 - mse: 9.6933 - val_loss: 13.2432 - val_mae: 2.6369 - val_mse: 13.2432\n",
      "Epoch 179/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.6458 - mae: 2.3846 - mse: 9.6458 - val_loss: 13.1210 - val_mae: 2.6255 - val_mse: 13.1210\n",
      "Epoch 180/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.6610 - mae: 2.3749 - mse: 9.6610 - val_loss: 13.2108 - val_mae: 2.6251 - val_mse: 13.2108\n",
      "Epoch 181/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 9.4920 - mae: 2.3652 - mse: 9.4920 - val_loss: 13.0527 - val_mae: 2.6225 - val_mse: 13.0527\n",
      "Epoch 182/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.4207 - mae: 2.3464 - mse: 9.4207 - val_loss: 12.9862 - val_mae: 2.6059 - val_mse: 12.9862\n",
      "Epoch 183/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.3742 - mae: 2.3478 - mse: 9.3742 - val_loss: 12.9047 - val_mae: 2.6044 - val_mse: 12.9047\n",
      "Epoch 184/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.3325 - mae: 2.3437 - mse: 9.3325 - val_loss: 12.8624 - val_mae: 2.6003 - val_mse: 12.8624\n",
      "Epoch 185/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.3119 - mae: 2.3429 - mse: 9.3119 - val_loss: 12.8291 - val_mae: 2.5893 - val_mse: 12.8291\n",
      "Epoch 186/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.2452 - mae: 2.3304 - mse: 9.2452 - val_loss: 12.7585 - val_mae: 2.5650 - val_mse: 12.7585\n",
      "Epoch 187/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.2231 - mae: 2.3330 - mse: 9.2231 - val_loss: 12.7926 - val_mae: 2.5911 - val_mse: 12.7926\n",
      "Epoch 188/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.1509 - mae: 2.3127 - mse: 9.1509 - val_loss: 12.6680 - val_mae: 2.5719 - val_mse: 12.6680\n",
      "Epoch 189/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.0994 - mae: 2.3082 - mse: 9.0994 - val_loss: 12.6331 - val_mae: 2.5721 - val_mse: 12.6331\n",
      "Epoch 190/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 9.0217 - mae: 2.2998 - mse: 9.0217 - val_loss: 12.5618 - val_mae: 2.5610 - val_mse: 12.5618\n",
      "Epoch 191/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 8.9923 - mae: 2.3005 - mse: 8.9923 - val_loss: 12.5289 - val_mae: 2.5442 - val_mse: 12.5289\n",
      "Epoch 192/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 8.9787 - mae: 2.2948 - mse: 8.9787 - val_loss: 12.3879 - val_mae: 2.5425 - val_mse: 12.3879\n",
      "Epoch 193/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 8.9885 - mae: 2.2967 - mse: 8.9885 - val_loss: 12.3716 - val_mae: 2.5422 - val_mse: 12.3716\n",
      "Epoch 194/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 8.8597 - mae: 2.2774 - mse: 8.8597 - val_loss: 12.4236 - val_mae: 2.5406 - val_mse: 12.4236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 8.8228 - mae: 2.2717 - mse: 8.8228 - val_loss: 12.3626 - val_mae: 2.5401 - val_mse: 12.3626\n",
      "Epoch 196/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.8217 - mae: 2.2760 - mse: 8.8217 - val_loss: 12.1882 - val_mae: 2.5073 - val_mse: 12.1882\n",
      "Epoch 197/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.7778 - mae: 2.2663 - mse: 8.7778 - val_loss: 12.1925 - val_mae: 2.5066 - val_mse: 12.1925\n",
      "Epoch 198/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.7939 - mae: 2.2748 - mse: 8.7939 - val_loss: 12.2399 - val_mae: 2.5234 - val_mse: 12.2399\n",
      "Epoch 199/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.6328 - mae: 2.2522 - mse: 8.6328 - val_loss: 12.1940 - val_mae: 2.5142 - val_mse: 12.1940\n",
      "Epoch 200/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.6126 - mae: 2.2459 - mse: 8.6126 - val_loss: 11.9978 - val_mae: 2.4896 - val_mse: 11.9978\n",
      "Epoch 201/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 8.5837 - mae: 2.2392 - mse: 8.5837 - val_loss: 11.9956 - val_mae: 2.5061 - val_mse: 11.9956\n",
      "Epoch 202/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.5362 - mae: 2.2454 - mse: 8.5362 - val_loss: 12.0304 - val_mae: 2.4887 - val_mse: 12.0304\n",
      "Epoch 203/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 8.5209 - mae: 2.2332 - mse: 8.5209 - val_loss: 11.9617 - val_mae: 2.4877 - val_mse: 11.9617\n",
      "Epoch 204/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.4884 - mae: 2.2276 - mse: 8.4884 - val_loss: 11.8758 - val_mae: 2.4685 - val_mse: 11.8758\n",
      "Epoch 205/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.4237 - mae: 2.2261 - mse: 8.4237 - val_loss: 11.8192 - val_mae: 2.4922 - val_mse: 11.8192\n",
      "Epoch 206/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.4585 - mae: 2.2300 - mse: 8.4585 - val_loss: 11.7530 - val_mae: 2.4606 - val_mse: 11.7530\n",
      "Epoch 207/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.3747 - mae: 2.2137 - mse: 8.3747 - val_loss: 11.6742 - val_mae: 2.4655 - val_mse: 11.6742\n",
      "Epoch 208/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.3237 - mae: 2.2065 - mse: 8.3237 - val_loss: 11.7048 - val_mae: 2.4618 - val_mse: 11.7048\n",
      "Epoch 209/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 8.2487 - mae: 2.1982 - mse: 8.2487 - val_loss: 11.5729 - val_mae: 2.4336 - val_mse: 11.5729\n",
      "Epoch 210/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.2389 - mae: 2.1873 - mse: 8.2389 - val_loss: 11.5187 - val_mae: 2.4391 - val_mse: 11.5187\n",
      "Epoch 211/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.2073 - mae: 2.1945 - mse: 8.2073 - val_loss: 11.5136 - val_mae: 2.4407 - val_mse: 11.5136\n",
      "Epoch 212/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.1656 - mae: 2.1918 - mse: 8.1656 - val_loss: 11.5512 - val_mae: 2.4520 - val_mse: 11.5512\n",
      "Epoch 213/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.1549 - mae: 2.1781 - mse: 8.1549 - val_loss: 11.4232 - val_mae: 2.4176 - val_mse: 11.4232\n",
      "Epoch 214/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.1702 - mae: 2.1986 - mse: 8.1702 - val_loss: 11.3665 - val_mae: 2.4245 - val_mse: 11.3665\n",
      "Epoch 215/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 8.1221 - mae: 2.1806 - mse: 8.1221 - val_loss: 11.4047 - val_mae: 2.4298 - val_mse: 11.4047\n",
      "Epoch 216/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 8.0160 - mae: 2.1659 - mse: 8.0160 - val_loss: 11.3006 - val_mae: 2.4039 - val_mse: 11.3006\n",
      "Epoch 217/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.9961 - mae: 2.1591 - mse: 7.9961 - val_loss: 11.2360 - val_mae: 2.4072 - val_mse: 11.2360\n",
      "Epoch 218/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.9564 - mae: 2.1572 - mse: 7.9564 - val_loss: 11.1857 - val_mae: 2.4003 - val_mse: 11.1857\n",
      "Epoch 219/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.9302 - mae: 2.1531 - mse: 7.9302 - val_loss: 11.2133 - val_mae: 2.3991 - val_mse: 11.2133\n",
      "Epoch 220/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.8898 - mae: 2.1426 - mse: 7.8898 - val_loss: 11.1822 - val_mae: 2.3863 - val_mse: 11.1822\n",
      "Epoch 221/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.9011 - mae: 2.1546 - mse: 7.9011 - val_loss: 11.0554 - val_mae: 2.3962 - val_mse: 11.0554\n",
      "Epoch 222/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.8681 - mae: 2.1409 - mse: 7.8681 - val_loss: 11.1471 - val_mae: 2.3945 - val_mse: 11.1471\n",
      "Epoch 223/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.8664 - mae: 2.1389 - mse: 7.8664 - val_loss: 11.0329 - val_mae: 2.3740 - val_mse: 11.0329\n",
      "Epoch 224/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 7.7931 - mae: 2.1291 - mse: 7.7931 - val_loss: 11.0188 - val_mae: 2.3960 - val_mse: 11.0188\n",
      "Epoch 225/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.7501 - mae: 2.1294 - mse: 7.7501 - val_loss: 11.0878 - val_mae: 2.3789 - val_mse: 11.0878\n",
      "Epoch 226/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.7983 - mae: 2.1428 - mse: 7.7983 - val_loss: 11.1056 - val_mae: 2.4140 - val_mse: 11.1056\n",
      "Epoch 227/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.8723 - mae: 2.1360 - mse: 7.8723 - val_loss: 10.9939 - val_mae: 2.3622 - val_mse: 10.9939\n",
      "Epoch 228/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.7303 - mae: 2.1172 - mse: 7.7303 - val_loss: 10.8816 - val_mae: 2.3832 - val_mse: 10.8816\n",
      "Epoch 229/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.6639 - mae: 2.1108 - mse: 7.6639 - val_loss: 10.9332 - val_mae: 2.3562 - val_mse: 10.9332\n",
      "Epoch 230/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.6113 - mae: 2.1007 - mse: 7.6113 - val_loss: 10.8223 - val_mae: 2.3648 - val_mse: 10.8223\n",
      "Epoch 231/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.6348 - mae: 2.1124 - mse: 7.6348 - val_loss: 10.8224 - val_mae: 2.3562 - val_mse: 10.8224\n",
      "Epoch 232/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.6017 - mae: 2.0995 - mse: 7.6017 - val_loss: 10.7882 - val_mae: 2.3650 - val_mse: 10.7882\n",
      "Epoch 233/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.5607 - mae: 2.0919 - mse: 7.5607 - val_loss: 10.7566 - val_mae: 2.3491 - val_mse: 10.7566\n",
      "Epoch 234/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.5343 - mae: 2.0994 - mse: 7.5343 - val_loss: 10.7855 - val_mae: 2.3546 - val_mse: 10.7855\n",
      "Epoch 235/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.6721 - mae: 2.1155 - mse: 7.6721 - val_loss: 10.8024 - val_mae: 2.3941 - val_mse: 10.8024\n",
      "Epoch 236/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.5010 - mae: 2.0995 - mse: 7.5010 - val_loss: 10.5983 - val_mae: 2.3346 - val_mse: 10.5983\n",
      "Epoch 237/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.4975 - mae: 2.0868 - mse: 7.4975 - val_loss: 10.7202 - val_mae: 2.3435 - val_mse: 10.7202\n",
      "Epoch 238/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.4206 - mae: 2.0709 - mse: 7.4206 - val_loss: 10.6632 - val_mae: 2.3612 - val_mse: 10.6632\n",
      "Epoch 239/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.4238 - mae: 2.0920 - mse: 7.4238 - val_loss: 10.5039 - val_mae: 2.3391 - val_mse: 10.5039\n",
      "Epoch 240/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.4270 - mae: 2.0722 - mse: 7.4270 - val_loss: 10.4371 - val_mae: 2.3275 - val_mse: 10.4371\n",
      "Epoch 241/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.3943 - mae: 2.0916 - mse: 7.3943 - val_loss: 10.5677 - val_mae: 2.3349 - val_mse: 10.5677\n",
      "Epoch 242/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.3258 - mae: 2.0626 - mse: 7.3258 - val_loss: 10.4582 - val_mae: 2.3406 - val_mse: 10.4582\n",
      "Epoch 243/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.2933 - mae: 2.0648 - mse: 7.2933 - val_loss: 10.4520 - val_mae: 2.3228 - val_mse: 10.4520\n",
      "Epoch 244/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 6ms/step - loss: 7.3187 - mae: 2.0567 - mse: 7.3187 - val_loss: 10.5233 - val_mae: 2.3512 - val_mse: 10.5233\n",
      "Epoch 245/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 7.2973 - mae: 2.0695 - mse: 7.2973 - val_loss: 10.5218 - val_mae: 2.3368 - val_mse: 10.5218\n",
      "Epoch 246/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.2761 - mae: 2.0571 - mse: 7.2761 - val_loss: 10.3620 - val_mae: 2.3221 - val_mse: 10.3620\n",
      "Epoch 247/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.1826 - mae: 2.0454 - mse: 7.1826 - val_loss: 10.3790 - val_mae: 2.3177 - val_mse: 10.3790\n",
      "Epoch 248/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.1575 - mae: 2.0412 - mse: 7.1575 - val_loss: 10.3528 - val_mae: 2.3137 - val_mse: 10.3528\n",
      "Epoch 249/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.1843 - mae: 2.0505 - mse: 7.1843 - val_loss: 10.3366 - val_mae: 2.3314 - val_mse: 10.3366\n",
      "Epoch 250/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 7.1367 - mae: 2.0390 - mse: 7.1367 - val_loss: 10.2638 - val_mae: 2.3088 - val_mse: 10.2638\n",
      "Epoch 251/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.1239 - mae: 2.0407 - mse: 7.1239 - val_loss: 10.3727 - val_mae: 2.3173 - val_mse: 10.3727\n",
      "Epoch 252/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.0883 - mae: 2.0370 - mse: 7.0883 - val_loss: 10.2082 - val_mae: 2.3255 - val_mse: 10.2082\n",
      "Epoch 253/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.0791 - mae: 2.0344 - mse: 7.0791 - val_loss: 10.2305 - val_mae: 2.3001 - val_mse: 10.2305\n",
      "Epoch 254/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.0347 - mae: 2.0232 - mse: 7.0347 - val_loss: 10.2819 - val_mae: 2.3158 - val_mse: 10.2819\n",
      "Epoch 255/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 7.0217 - mae: 2.0250 - mse: 7.0217 - val_loss: 10.1994 - val_mae: 2.3025 - val_mse: 10.1994\n",
      "Epoch 256/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.9709 - mae: 2.0120 - mse: 6.9709 - val_loss: 10.1094 - val_mae: 2.2952 - val_mse: 10.1094\n",
      "Epoch 257/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.9564 - mae: 2.0129 - mse: 6.9564 - val_loss: 10.2802 - val_mae: 2.3079 - val_mse: 10.2802\n",
      "Epoch 258/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.9233 - mae: 2.0146 - mse: 6.9233 - val_loss: 10.1321 - val_mae: 2.2885 - val_mse: 10.1321\n",
      "Epoch 259/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.9114 - mae: 2.0052 - mse: 6.9114 - val_loss: 10.1454 - val_mae: 2.3152 - val_mse: 10.1454\n",
      "Epoch 260/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.9233 - mae: 2.0133 - mse: 6.9233 - val_loss: 10.1389 - val_mae: 2.2966 - val_mse: 10.1389\n",
      "Epoch 261/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.9690 - mae: 2.0198 - mse: 6.9690 - val_loss: 10.0794 - val_mae: 2.2884 - val_mse: 10.0794\n",
      "Epoch 262/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.9187 - mae: 2.0104 - mse: 6.9187 - val_loss: 10.1876 - val_mae: 2.3124 - val_mse: 10.1876\n",
      "Epoch 263/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.8622 - mae: 1.9946 - mse: 6.8622 - val_loss: 10.2040 - val_mae: 2.3088 - val_mse: 10.2040\n",
      "Epoch 264/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.9588 - mae: 2.0152 - mse: 6.9588 - val_loss: 10.0342 - val_mae: 2.3018 - val_mse: 10.0342\n",
      "Epoch 265/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.7937 - mae: 1.9865 - mse: 6.7937 - val_loss: 9.9882 - val_mae: 2.2739 - val_mse: 9.9882\n",
      "Epoch 266/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.8241 - mae: 1.9876 - mse: 6.8241 - val_loss: 10.0570 - val_mae: 2.3085 - val_mse: 10.0570\n",
      "Epoch 267/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.7751 - mae: 1.9872 - mse: 6.7751 - val_loss: 9.9856 - val_mae: 2.2738 - val_mse: 9.9856\n",
      "Epoch 268/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.7711 - mae: 1.9841 - mse: 6.7711 - val_loss: 9.9396 - val_mae: 2.2770 - val_mse: 9.9396\n",
      "Epoch 269/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.7647 - mae: 1.9865 - mse: 6.7647 - val_loss: 10.0879 - val_mae: 2.2909 - val_mse: 10.0879\n",
      "Epoch 270/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.7220 - mae: 1.9730 - mse: 6.7220 - val_loss: 10.0102 - val_mae: 2.2666 - val_mse: 10.0102\n",
      "Epoch 271/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.8476 - mae: 1.9904 - mse: 6.8476 - val_loss: 10.0451 - val_mae: 2.2809 - val_mse: 10.0451\n",
      "Epoch 272/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.6721 - mae: 1.9630 - mse: 6.6721 - val_loss: 9.8245 - val_mae: 2.2589 - val_mse: 9.8245\n",
      "Epoch 273/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.7153 - mae: 1.9758 - mse: 6.7153 - val_loss: 9.9518 - val_mae: 2.2811 - val_mse: 9.9518\n",
      "Epoch 274/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.6917 - mae: 1.9743 - mse: 6.6917 - val_loss: 10.0117 - val_mae: 2.2644 - val_mse: 10.0117\n",
      "Epoch 275/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.7171 - mae: 1.9659 - mse: 6.7171 - val_loss: 9.8079 - val_mae: 2.2656 - val_mse: 9.8079\n",
      "Epoch 276/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.6260 - mae: 1.9607 - mse: 6.6260 - val_loss: 10.1479 - val_mae: 2.3134 - val_mse: 10.1479\n",
      "Epoch 277/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.6366 - mae: 1.9739 - mse: 6.6366 - val_loss: 9.8751 - val_mae: 2.2582 - val_mse: 9.8751\n",
      "Epoch 278/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.6177 - mae: 1.9551 - mse: 6.6177 - val_loss: 9.7870 - val_mae: 2.2496 - val_mse: 9.7870\n",
      "Epoch 279/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.6218 - mae: 1.9681 - mse: 6.6218 - val_loss: 10.1019 - val_mae: 2.3137 - val_mse: 10.1019\n",
      "Epoch 280/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.6562 - mae: 1.9571 - mse: 6.6562 - val_loss: 9.7818 - val_mae: 2.2487 - val_mse: 9.7818\n",
      "Epoch 281/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.5535 - mae: 1.9521 - mse: 6.5535 - val_loss: 9.7622 - val_mae: 2.2548 - val_mse: 9.7622\n",
      "Epoch 282/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.5068 - mae: 1.9362 - mse: 6.5068 - val_loss: 9.8216 - val_mae: 2.2529 - val_mse: 9.8216\n",
      "Epoch 283/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.5416 - mae: 1.9468 - mse: 6.5416 - val_loss: 9.7882 - val_mae: 2.2420 - val_mse: 9.7882\n",
      "Epoch 284/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.4701 - mae: 1.9285 - mse: 6.4701 - val_loss: 9.7012 - val_mae: 2.2522 - val_mse: 9.7012\n",
      "Epoch 285/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.4445 - mae: 1.9258 - mse: 6.4445 - val_loss: 9.7300 - val_mae: 2.2400 - val_mse: 9.7300\n",
      "Epoch 286/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.3904 - mae: 1.9238 - mse: 6.3904 - val_loss: 9.7123 - val_mae: 2.2458 - val_mse: 9.7123\n",
      "Epoch 287/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.4379 - mae: 1.9193 - mse: 6.4379 - val_loss: 9.7795 - val_mae: 2.2450 - val_mse: 9.7795\n",
      "Epoch 288/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.4518 - mae: 1.9216 - mse: 6.4518 - val_loss: 9.6965 - val_mae: 2.2572 - val_mse: 9.6965\n",
      "Epoch 289/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.3906 - mae: 1.9198 - mse: 6.3906 - val_loss: 9.6601 - val_mae: 2.2329 - val_mse: 9.6601\n",
      "Epoch 290/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 6.3549 - mae: 1.9147 - mse: 6.3549 - val_loss: 9.6368 - val_mae: 2.2292 - val_mse: 9.6368\n",
      "Epoch 291/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.3441 - mae: 1.9051 - mse: 6.3441 - val_loss: 9.6977 - val_mae: 2.2502 - val_mse: 9.6977\n",
      "Epoch 292/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.3692 - mae: 1.9256 - mse: 6.3692 - val_loss: 9.6883 - val_mae: 2.2363 - val_mse: 9.6883\n",
      "Epoch 293/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 7ms/step - loss: 6.3962 - mae: 1.9251 - mse: 6.3962 - val_loss: 9.6632 - val_mae: 2.2594 - val_mse: 9.6632\n",
      "Epoch 294/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.4094 - mae: 1.9242 - mse: 6.4094 - val_loss: 9.5974 - val_mae: 2.2412 - val_mse: 9.5974\n",
      "Epoch 295/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.3048 - mae: 1.8998 - mse: 6.3048 - val_loss: 9.5378 - val_mae: 2.2237 - val_mse: 9.5378\n",
      "Epoch 296/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.2556 - mae: 1.8852 - mse: 6.2556 - val_loss: 9.5060 - val_mae: 2.2331 - val_mse: 9.5060\n",
      "Epoch 297/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.2261 - mae: 1.8916 - mse: 6.2261 - val_loss: 9.4390 - val_mae: 2.2211 - val_mse: 9.4390\n",
      "Epoch 298/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.1857 - mae: 1.8804 - mse: 6.1857 - val_loss: 9.5219 - val_mae: 2.2250 - val_mse: 9.5219\n",
      "Epoch 299/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.2228 - mae: 1.8833 - mse: 6.2228 - val_loss: 9.5316 - val_mae: 2.2046 - val_mse: 9.5316\n",
      "Epoch 300/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.2046 - mae: 1.8842 - mse: 6.2046 - val_loss: 9.4542 - val_mae: 2.2296 - val_mse: 9.4542\n",
      "Epoch 301/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.2418 - mae: 1.8950 - mse: 6.2418 - val_loss: 9.4380 - val_mae: 2.2139 - val_mse: 9.4380\n",
      "Epoch 302/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.1746 - mae: 1.8798 - mse: 6.1746 - val_loss: 9.4151 - val_mae: 2.2332 - val_mse: 9.4151\n",
      "Epoch 303/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.2065 - mae: 1.8845 - mse: 6.2065 - val_loss: 9.5441 - val_mae: 2.2242 - val_mse: 9.5441\n",
      "Epoch 304/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.2714 - mae: 1.9052 - mse: 6.2714 - val_loss: 9.3469 - val_mae: 2.2138 - val_mse: 9.3469\n",
      "Epoch 305/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.1879 - mae: 1.8842 - mse: 6.1879 - val_loss: 9.3507 - val_mae: 2.1984 - val_mse: 9.3507\n",
      "Epoch 306/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.1737 - mae: 1.8912 - mse: 6.1737 - val_loss: 9.2686 - val_mae: 2.2051 - val_mse: 9.2686\n",
      "Epoch 307/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.1543 - mae: 1.8722 - mse: 6.1543 - val_loss: 9.3790 - val_mae: 2.2054 - val_mse: 9.3790\n",
      "Epoch 308/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.0986 - mae: 1.8692 - mse: 6.0986 - val_loss: 9.3722 - val_mae: 2.1977 - val_mse: 9.3722\n",
      "Epoch 309/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.1218 - mae: 1.8580 - mse: 6.1218 - val_loss: 9.2357 - val_mae: 2.1800 - val_mse: 9.2357\n",
      "Epoch 310/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.1047 - mae: 1.8762 - mse: 6.1047 - val_loss: 9.3556 - val_mae: 2.2141 - val_mse: 9.3556\n",
      "Epoch 311/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.0565 - mae: 1.8615 - mse: 6.0565 - val_loss: 9.2724 - val_mae: 2.2023 - val_mse: 9.2724\n",
      "Epoch 312/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 6.0840 - mae: 1.8739 - mse: 6.0840 - val_loss: 9.3362 - val_mae: 2.2022 - val_mse: 9.3362\n",
      "Epoch 313/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.1030 - mae: 1.8688 - mse: 6.1030 - val_loss: 9.1822 - val_mae: 2.1817 - val_mse: 9.1822\n",
      "Epoch 314/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.0788 - mae: 1.8611 - mse: 6.0788 - val_loss: 9.2582 - val_mae: 2.2064 - val_mse: 9.2582\n",
      "Epoch 315/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.0624 - mae: 1.8586 - mse: 6.0624 - val_loss: 9.2507 - val_mae: 2.1858 - val_mse: 9.2507\n",
      "Epoch 316/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 6.0984 - mae: 1.8735 - mse: 6.0984 - val_loss: 9.2547 - val_mae: 2.2005 - val_mse: 9.2547\n",
      "Epoch 317/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.9979 - mae: 1.8540 - mse: 5.9979 - val_loss: 9.1391 - val_mae: 2.1778 - val_mse: 9.1391\n",
      "Epoch 318/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.9843 - mae: 1.8528 - mse: 5.9843 - val_loss: 9.0599 - val_mae: 2.1619 - val_mse: 9.0599\n",
      "Epoch 319/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5.9973 - mae: 1.8553 - mse: 5.9973 - val_loss: 9.1342 - val_mae: 2.1865 - val_mse: 9.1342\n",
      "Epoch 320/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.9511 - mae: 1.8407 - mse: 5.9511 - val_loss: 9.0584 - val_mae: 2.1628 - val_mse: 9.0584\n",
      "Epoch 321/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.9481 - mae: 1.8456 - mse: 5.9481 - val_loss: 9.1029 - val_mae: 2.1784 - val_mse: 9.1029\n",
      "Epoch 322/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.9130 - mae: 1.8322 - mse: 5.9130 - val_loss: 9.0401 - val_mae: 2.1632 - val_mse: 9.0401\n",
      "Epoch 323/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.8808 - mae: 1.8290 - mse: 5.8808 - val_loss: 9.1479 - val_mae: 2.1703 - val_mse: 9.1479\n",
      "Epoch 324/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.9361 - mae: 1.8404 - mse: 5.9361 - val_loss: 9.0567 - val_mae: 2.1671 - val_mse: 9.0567\n",
      "Epoch 325/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.8998 - mae: 1.8367 - mse: 5.8998 - val_loss: 9.0243 - val_mae: 2.1581 - val_mse: 9.0243\n",
      "Epoch 326/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.9059 - mae: 1.8326 - mse: 5.9059 - val_loss: 9.0168 - val_mae: 2.1687 - val_mse: 9.0168\n",
      "Epoch 327/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.9215 - mae: 1.8453 - mse: 5.9215 - val_loss: 9.0774 - val_mae: 2.1681 - val_mse: 9.0774\n",
      "Epoch 328/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.9586 - mae: 1.8437 - mse: 5.9586 - val_loss: 9.0184 - val_mae: 2.1625 - val_mse: 9.0184\n",
      "Epoch 329/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.8943 - mae: 1.8420 - mse: 5.8943 - val_loss: 9.0436 - val_mae: 2.1684 - val_mse: 9.0436\n",
      "Epoch 330/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.8287 - mae: 1.8290 - mse: 5.8287 - val_loss: 9.1717 - val_mae: 2.1789 - val_mse: 9.1717\n",
      "Epoch 331/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.8390 - mae: 1.8257 - mse: 5.8390 - val_loss: 8.9975 - val_mae: 2.1533 - val_mse: 8.9975\n",
      "Epoch 332/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.8815 - mae: 1.8292 - mse: 5.8815 - val_loss: 8.9106 - val_mae: 2.1525 - val_mse: 8.9106\n",
      "Epoch 333/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.8418 - mae: 1.8251 - mse: 5.8418 - val_loss: 8.9694 - val_mae: 2.1369 - val_mse: 8.9694\n",
      "Epoch 334/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.8040 - mae: 1.8155 - mse: 5.8040 - val_loss: 8.9451 - val_mae: 2.1483 - val_mse: 8.9451\n",
      "Epoch 335/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.7940 - mae: 1.8137 - mse: 5.7940 - val_loss: 8.8278 - val_mae: 2.1395 - val_mse: 8.8278\n",
      "Epoch 336/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.7461 - mae: 1.8036 - mse: 5.7461 - val_loss: 8.8953 - val_mae: 2.1277 - val_mse: 8.8953\n",
      "Epoch 337/500\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 5.7744 - mae: 1.8079 - mse: 5.7744 - val_loss: 8.9071 - val_mae: 2.1596 - val_mse: 8.9071\n",
      "Epoch 338/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.7555 - mae: 1.8159 - mse: 5.7555 - val_loss: 8.8678 - val_mae: 2.1344 - val_mse: 8.8678\n",
      "Epoch 339/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.8135 - mae: 1.8197 - mse: 5.8135 - val_loss: 8.9956 - val_mae: 2.1432 - val_mse: 8.9956\n",
      "Epoch 340/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5.7708 - mae: 1.8203 - mse: 5.7708 - val_loss: 8.9316 - val_mae: 2.1485 - val_mse: 8.9316\n",
      "Epoch 341/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5.6972 - mae: 1.8007 - mse: 5.6972 - val_loss: 8.7999 - val_mae: 2.1182 - val_mse: 8.7999\n",
      "Epoch 342/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5.6990 - mae: 1.7949 - mse: 5.6990 - val_loss: 8.7553 - val_mae: 2.1276 - val_mse: 8.7553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5.7232 - mae: 1.8027 - mse: 5.7232 - val_loss: 8.7957 - val_mae: 2.1342 - val_mse: 8.7957\n",
      "Epoch 344/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.7111 - mae: 1.7975 - mse: 5.7111 - val_loss: 8.8518 - val_mae: 2.1457 - val_mse: 8.8518\n",
      "Epoch 345/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.6996 - mae: 1.8039 - mse: 5.6996 - val_loss: 8.7695 - val_mae: 2.1129 - val_mse: 8.7695\n",
      "Epoch 346/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.6510 - mae: 1.7934 - mse: 5.6510 - val_loss: 8.7013 - val_mae: 2.1206 - val_mse: 8.7013\n",
      "Epoch 347/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.6485 - mae: 1.7931 - mse: 5.6485 - val_loss: 8.6991 - val_mae: 2.1262 - val_mse: 8.6991\n",
      "Epoch 348/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.6338 - mae: 1.7832 - mse: 5.6338 - val_loss: 8.7542 - val_mae: 2.1040 - val_mse: 8.7542\n",
      "Epoch 349/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5.7129 - mae: 1.8060 - mse: 5.7129 - val_loss: 8.8693 - val_mae: 2.1519 - val_mse: 8.8693\n",
      "Epoch 350/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.6986 - mae: 1.7946 - mse: 5.6986 - val_loss: 8.7386 - val_mae: 2.1309 - val_mse: 8.7386\n",
      "Epoch 351/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5.6527 - mae: 1.7976 - mse: 5.6527 - val_loss: 8.6189 - val_mae: 2.1096 - val_mse: 8.6189\n",
      "Epoch 352/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.5373 - mae: 1.7650 - mse: 5.5373 - val_loss: 8.6336 - val_mae: 2.1004 - val_mse: 8.6336\n",
      "Epoch 353/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.5226 - mae: 1.7627 - mse: 5.5226 - val_loss: 8.7253 - val_mae: 2.1235 - val_mse: 8.7253\n",
      "Epoch 354/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.5208 - mae: 1.7588 - mse: 5.5208 - val_loss: 8.7474 - val_mae: 2.1348 - val_mse: 8.7474\n",
      "Epoch 355/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.5538 - mae: 1.7674 - mse: 5.5538 - val_loss: 8.6510 - val_mae: 2.0998 - val_mse: 8.6510\n",
      "Epoch 356/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.4911 - mae: 1.7644 - mse: 5.4911 - val_loss: 8.6607 - val_mae: 2.1011 - val_mse: 8.6607\n",
      "Epoch 357/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.4465 - mae: 1.7453 - mse: 5.4465 - val_loss: 8.5791 - val_mae: 2.1070 - val_mse: 8.5791\n",
      "Epoch 358/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.5327 - mae: 1.7638 - mse: 5.5327 - val_loss: 8.7085 - val_mae: 2.1228 - val_mse: 8.7085\n",
      "Epoch 359/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.5040 - mae: 1.7621 - mse: 5.5040 - val_loss: 8.6062 - val_mae: 2.1000 - val_mse: 8.6062\n",
      "Epoch 360/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.4311 - mae: 1.7486 - mse: 5.4311 - val_loss: 8.5595 - val_mae: 2.0737 - val_mse: 8.5595\n",
      "Epoch 361/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.4086 - mae: 1.7488 - mse: 5.4086 - val_loss: 8.4943 - val_mae: 2.0839 - val_mse: 8.4943\n",
      "Epoch 362/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.4081 - mae: 1.7383 - mse: 5.4081 - val_loss: 8.5302 - val_mae: 2.0718 - val_mse: 8.5302\n",
      "Epoch 363/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.3814 - mae: 1.7337 - mse: 5.3814 - val_loss: 8.5549 - val_mae: 2.0979 - val_mse: 8.5549\n",
      "Epoch 364/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.3689 - mae: 1.7362 - mse: 5.3689 - val_loss: 8.4190 - val_mae: 2.0697 - val_mse: 8.4190\n",
      "Epoch 365/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.4327 - mae: 1.7520 - mse: 5.4327 - val_loss: 8.5099 - val_mae: 2.0853 - val_mse: 8.5099\n",
      "Epoch 366/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.4027 - mae: 1.7521 - mse: 5.4027 - val_loss: 8.5021 - val_mae: 2.0840 - val_mse: 8.5021\n",
      "Epoch 367/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.3695 - mae: 1.7409 - mse: 5.3695 - val_loss: 8.4812 - val_mae: 2.0732 - val_mse: 8.4812\n",
      "Epoch 368/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.3788 - mae: 1.7383 - mse: 5.3788 - val_loss: 8.4444 - val_mae: 2.0743 - val_mse: 8.4444\n",
      "Epoch 369/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.3594 - mae: 1.7359 - mse: 5.3594 - val_loss: 8.4900 - val_mae: 2.0645 - val_mse: 8.4900\n",
      "Epoch 370/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.3253 - mae: 1.7326 - mse: 5.3253 - val_loss: 8.4256 - val_mae: 2.0683 - val_mse: 8.4256\n",
      "Epoch 371/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.4119 - mae: 1.7349 - mse: 5.4119 - val_loss: 8.5248 - val_mae: 2.0911 - val_mse: 8.5248\n",
      "Epoch 372/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.4845 - mae: 1.7633 - mse: 5.4845 - val_loss: 8.4143 - val_mae: 2.0698 - val_mse: 8.4143\n",
      "Epoch 373/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5.3114 - mae: 1.7301 - mse: 5.3114 - val_loss: 8.5107 - val_mae: 2.0901 - val_mse: 8.5107\n",
      "Epoch 374/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.3089 - mae: 1.7250 - mse: 5.3089 - val_loss: 8.4046 - val_mae: 2.0441 - val_mse: 8.4046\n",
      "Epoch 375/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.3600 - mae: 1.7381 - mse: 5.3600 - val_loss: 8.5310 - val_mae: 2.1150 - val_mse: 8.5310\n",
      "Epoch 376/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.3657 - mae: 1.7508 - mse: 5.3657 - val_loss: 8.3938 - val_mae: 2.0691 - val_mse: 8.3938\n",
      "Epoch 377/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.4400 - mae: 1.7442 - mse: 5.4400 - val_loss: 8.3228 - val_mae: 2.0608 - val_mse: 8.3228\n",
      "Epoch 378/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.2781 - mae: 1.7286 - mse: 5.2781 - val_loss: 8.3928 - val_mae: 2.0579 - val_mse: 8.3928\n",
      "Epoch 379/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.2709 - mae: 1.7156 - mse: 5.2709 - val_loss: 8.3611 - val_mae: 2.0515 - val_mse: 8.3611\n",
      "Epoch 380/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.2078 - mae: 1.7140 - mse: 5.2078 - val_loss: 8.3456 - val_mae: 2.0740 - val_mse: 8.3456\n",
      "Epoch 381/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.2713 - mae: 1.7245 - mse: 5.2713 - val_loss: 8.4502 - val_mae: 2.0747 - val_mse: 8.4502\n",
      "Epoch 382/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.2767 - mae: 1.7134 - mse: 5.2767 - val_loss: 8.3005 - val_mae: 2.0533 - val_mse: 8.3005\n",
      "Epoch 383/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.1777 - mae: 1.7106 - mse: 5.1777 - val_loss: 8.3844 - val_mae: 2.0616 - val_mse: 8.3844\n",
      "Epoch 384/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.1232 - mae: 1.6920 - mse: 5.1232 - val_loss: 8.3108 - val_mae: 2.0514 - val_mse: 8.3108\n",
      "Epoch 385/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.2088 - mae: 1.7049 - mse: 5.2088 - val_loss: 8.3998 - val_mae: 2.0644 - val_mse: 8.3998\n",
      "Epoch 386/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.2308 - mae: 1.7077 - mse: 5.2308 - val_loss: 8.3541 - val_mae: 2.0772 - val_mse: 8.3541\n",
      "Epoch 387/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.1818 - mae: 1.7117 - mse: 5.1818 - val_loss: 8.2245 - val_mae: 2.0234 - val_mse: 8.2245\n",
      "Epoch 388/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.1511 - mae: 1.6990 - mse: 5.1511 - val_loss: 8.2501 - val_mae: 2.0431 - val_mse: 8.2501\n",
      "Epoch 389/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.1231 - mae: 1.6915 - mse: 5.1231 - val_loss: 8.3963 - val_mae: 2.0763 - val_mse: 8.3963\n",
      "Epoch 390/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.1660 - mae: 1.7076 - mse: 5.1660 - val_loss: 8.2123 - val_mae: 2.0192 - val_mse: 8.2123\n",
      "Epoch 391/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.1909 - mae: 1.7013 - mse: 5.1909 - val_loss: 8.2232 - val_mae: 2.0313 - val_mse: 8.2232\n",
      "Epoch 392/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.1434 - mae: 1.7077 - mse: 5.1434 - val_loss: 8.2126 - val_mae: 2.0190 - val_mse: 8.2126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.1331 - mae: 1.6951 - mse: 5.1331 - val_loss: 8.1197 - val_mae: 2.0187 - val_mse: 8.1197\n",
      "Epoch 394/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.0737 - mae: 1.6869 - mse: 5.0737 - val_loss: 8.2318 - val_mae: 2.0468 - val_mse: 8.2318\n",
      "Epoch 395/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.0802 - mae: 1.6759 - mse: 5.0802 - val_loss: 8.1987 - val_mae: 2.0234 - val_mse: 8.1987\n",
      "Epoch 396/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.0410 - mae: 1.6744 - mse: 5.0410 - val_loss: 8.1196 - val_mae: 2.0247 - val_mse: 8.1196\n",
      "Epoch 397/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.0466 - mae: 1.6811 - mse: 5.0466 - val_loss: 8.2342 - val_mae: 2.0255 - val_mse: 8.2342\n",
      "Epoch 398/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.0118 - mae: 1.6656 - mse: 5.0118 - val_loss: 8.2420 - val_mae: 2.0338 - val_mse: 8.2420\n",
      "Epoch 399/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.0125 - mae: 1.6720 - mse: 5.0125 - val_loss: 8.1252 - val_mae: 2.0083 - val_mse: 8.1252\n",
      "Epoch 400/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.0383 - mae: 1.6863 - mse: 5.0383 - val_loss: 8.1647 - val_mae: 2.0232 - val_mse: 8.1647\n",
      "Epoch 401/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5.0384 - mae: 1.6772 - mse: 5.0384 - val_loss: 8.2073 - val_mae: 2.0218 - val_mse: 8.2073\n",
      "Epoch 402/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4.9892 - mae: 1.6701 - mse: 4.9892 - val_loss: 8.2365 - val_mae: 2.0343 - val_mse: 8.2365\n",
      "Epoch 403/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.9866 - mae: 1.6635 - mse: 4.9866 - val_loss: 8.1227 - val_mae: 1.9991 - val_mse: 8.1227\n",
      "Epoch 404/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5.0634 - mae: 1.6882 - mse: 5.0634 - val_loss: 8.0521 - val_mae: 2.0059 - val_mse: 8.0521\n",
      "Epoch 405/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 5.0102 - mae: 1.6682 - mse: 5.0102 - val_loss: 8.1642 - val_mae: 2.0240 - val_mse: 8.1642\n",
      "Epoch 406/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.0126 - mae: 1.6640 - mse: 5.0126 - val_loss: 8.1225 - val_mae: 2.0048 - val_mse: 8.1225\n",
      "Epoch 407/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.1502 - mae: 1.7167 - mse: 5.1502 - val_loss: 8.0008 - val_mae: 1.9887 - val_mse: 8.0008\n",
      "Epoch 408/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8931 - mae: 1.6667 - mse: 4.8931 - val_loss: 8.2158 - val_mae: 2.0307 - val_mse: 8.2158\n",
      "Epoch 409/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.9453 - mae: 1.6533 - mse: 4.9453 - val_loss: 8.0832 - val_mae: 2.0118 - val_mse: 8.0832\n",
      "Epoch 410/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4.9446 - mae: 1.6622 - mse: 4.9446 - val_loss: 7.9838 - val_mae: 1.9865 - val_mse: 7.9838\n",
      "Epoch 411/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8793 - mae: 1.6429 - mse: 4.8793 - val_loss: 8.3418 - val_mae: 2.0459 - val_mse: 8.3418\n",
      "Epoch 412/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.0519 - mae: 1.6745 - mse: 5.0519 - val_loss: 8.2318 - val_mae: 2.0472 - val_mse: 8.2318\n",
      "Epoch 413/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.0073 - mae: 1.6839 - mse: 5.0073 - val_loss: 7.9809 - val_mae: 1.9788 - val_mse: 7.9809\n",
      "Epoch 414/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.0089 - mae: 1.6758 - mse: 5.0089 - val_loss: 8.0883 - val_mae: 2.0082 - val_mse: 8.0883\n",
      "Epoch 415/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8343 - mae: 1.6509 - mse: 4.8343 - val_loss: 8.1116 - val_mae: 2.0077 - val_mse: 8.1116\n",
      "Epoch 416/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8568 - mae: 1.6456 - mse: 4.8568 - val_loss: 7.9720 - val_mae: 1.9719 - val_mse: 7.9720\n",
      "Epoch 417/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8500 - mae: 1.6439 - mse: 4.8500 - val_loss: 8.0929 - val_mae: 2.0151 - val_mse: 8.0929\n",
      "Epoch 418/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.9321 - mae: 1.6684 - mse: 4.9321 - val_loss: 8.1820 - val_mae: 2.0322 - val_mse: 8.1820\n",
      "Epoch 419/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.9104 - mae: 1.6566 - mse: 4.9104 - val_loss: 8.0044 - val_mae: 1.9871 - val_mse: 8.0044\n",
      "Epoch 420/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4.8792 - mae: 1.6499 - mse: 4.8792 - val_loss: 7.9727 - val_mae: 1.9741 - val_mse: 7.9727\n",
      "Epoch 421/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4.8562 - mae: 1.6358 - mse: 4.8562 - val_loss: 8.0783 - val_mae: 1.9902 - val_mse: 8.0783\n",
      "Epoch 422/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8442 - mae: 1.6497 - mse: 4.8442 - val_loss: 8.1044 - val_mae: 2.0257 - val_mse: 8.1044\n",
      "Epoch 423/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8748 - mae: 1.6445 - mse: 4.8748 - val_loss: 8.1638 - val_mae: 2.0132 - val_mse: 8.1638\n",
      "Epoch 424/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8685 - mae: 1.6642 - mse: 4.8685 - val_loss: 8.0566 - val_mae: 2.0008 - val_mse: 8.0566\n",
      "Epoch 425/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8115 - mae: 1.6330 - mse: 4.8115 - val_loss: 7.9804 - val_mae: 1.9762 - val_mse: 7.9804\n",
      "Epoch 426/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.7945 - mae: 1.6257 - mse: 4.7945 - val_loss: 7.9082 - val_mae: 1.9639 - val_mse: 7.9082\n",
      "Epoch 427/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8203 - mae: 1.6322 - mse: 4.8203 - val_loss: 8.1089 - val_mae: 1.9956 - val_mse: 8.1089\n",
      "Epoch 428/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4.8792 - mae: 1.6559 - mse: 4.8792 - val_loss: 7.9527 - val_mae: 1.9768 - val_mse: 7.9527\n",
      "Epoch 429/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.7355 - mae: 1.6339 - mse: 4.7355 - val_loss: 8.1731 - val_mae: 2.0250 - val_mse: 8.1731\n",
      "Epoch 430/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.9006 - mae: 1.6536 - mse: 4.9006 - val_loss: 8.0121 - val_mae: 1.9878 - val_mse: 8.0121\n",
      "Epoch 431/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.7053 - mae: 1.6227 - mse: 4.7053 - val_loss: 8.0423 - val_mae: 2.0179 - val_mse: 8.0423\n",
      "Epoch 432/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8070 - mae: 1.6357 - mse: 4.8070 - val_loss: 7.9626 - val_mae: 1.9965 - val_mse: 7.9626\n",
      "Epoch 433/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.9170 - mae: 1.6697 - mse: 4.9170 - val_loss: 8.0660 - val_mae: 1.9885 - val_mse: 8.0660\n",
      "Epoch 434/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8614 - mae: 1.6435 - mse: 4.8614 - val_loss: 8.0075 - val_mae: 1.9958 - val_mse: 8.0075\n",
      "Epoch 435/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.7813 - mae: 1.6386 - mse: 4.7813 - val_loss: 7.9128 - val_mae: 1.9716 - val_mse: 7.9128\n",
      "Epoch 436/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6843 - mae: 1.6128 - mse: 4.6843 - val_loss: 7.8193 - val_mae: 1.9696 - val_mse: 7.8193\n",
      "Epoch 437/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.7620 - mae: 1.6353 - mse: 4.7620 - val_loss: 7.9444 - val_mae: 1.9857 - val_mse: 7.9444\n",
      "Epoch 438/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.7038 - mae: 1.6193 - mse: 4.7038 - val_loss: 7.8968 - val_mae: 1.9904 - val_mse: 7.8968\n",
      "Epoch 439/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6821 - mae: 1.6173 - mse: 4.6821 - val_loss: 7.8245 - val_mae: 1.9523 - val_mse: 7.8245\n",
      "Epoch 440/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6433 - mae: 1.6005 - mse: 4.6433 - val_loss: 7.8611 - val_mae: 1.9675 - val_mse: 7.8611\n",
      "Epoch 441/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6471 - mae: 1.6040 - mse: 4.6471 - val_loss: 7.9480 - val_mae: 1.9824 - val_mse: 7.9480\n",
      "Epoch 442/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6924 - mae: 1.6091 - mse: 4.6924 - val_loss: 8.0305 - val_mae: 2.0069 - val_mse: 8.0305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 443/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.7347 - mae: 1.6393 - mse: 4.7347 - val_loss: 7.8772 - val_mae: 1.9730 - val_mse: 7.8772\n",
      "Epoch 444/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6179 - mae: 1.5966 - mse: 4.6179 - val_loss: 7.7398 - val_mae: 1.9562 - val_mse: 7.7398\n",
      "Epoch 445/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6595 - mae: 1.6145 - mse: 4.6595 - val_loss: 7.8434 - val_mae: 1.9625 - val_mse: 7.8434\n",
      "Epoch 446/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5931 - mae: 1.5923 - mse: 4.5931 - val_loss: 7.7599 - val_mae: 1.9572 - val_mse: 7.7599\n",
      "Epoch 447/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5834 - mae: 1.5974 - mse: 4.5834 - val_loss: 7.7793 - val_mae: 1.9669 - val_mse: 7.7793\n",
      "Epoch 448/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6345 - mae: 1.6091 - mse: 4.6345 - val_loss: 7.8714 - val_mae: 1.9835 - val_mse: 7.8714\n",
      "Epoch 449/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5697 - mae: 1.5909 - mse: 4.5697 - val_loss: 7.8443 - val_mae: 1.9685 - val_mse: 7.8443\n",
      "Epoch 450/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6306 - mae: 1.6005 - mse: 4.6306 - val_loss: 7.7955 - val_mae: 1.9586 - val_mse: 7.7955\n",
      "Epoch 451/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5574 - mae: 1.5891 - mse: 4.5574 - val_loss: 7.7432 - val_mae: 1.9572 - val_mse: 7.7432\n",
      "Epoch 452/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6531 - mae: 1.6123 - mse: 4.6531 - val_loss: 7.8369 - val_mae: 1.9631 - val_mse: 7.8369\n",
      "Epoch 453/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5536 - mae: 1.5848 - mse: 4.5536 - val_loss: 7.7889 - val_mae: 1.9722 - val_mse: 7.7889\n",
      "Epoch 454/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5528 - mae: 1.5817 - mse: 4.5528 - val_loss: 7.9438 - val_mae: 2.0104 - val_mse: 7.9438\n",
      "Epoch 455/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.8078 - mae: 1.6334 - mse: 4.8078 - val_loss: 7.9947 - val_mae: 2.0216 - val_mse: 7.9947\n",
      "Epoch 456/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5922 - mae: 1.6037 - mse: 4.5922 - val_loss: 7.7759 - val_mae: 1.9552 - val_mse: 7.7759\n",
      "Epoch 457/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5535 - mae: 1.5903 - mse: 4.5535 - val_loss: 7.7301 - val_mae: 1.9585 - val_mse: 7.7301\n",
      "Epoch 458/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5759 - mae: 1.5789 - mse: 4.5759 - val_loss: 7.7635 - val_mae: 1.9565 - val_mse: 7.7635\n",
      "Epoch 459/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5931 - mae: 1.6024 - mse: 4.5931 - val_loss: 7.7510 - val_mae: 1.9616 - val_mse: 7.7510\n",
      "Epoch 460/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5494 - mae: 1.5891 - mse: 4.5494 - val_loss: 7.6544 - val_mae: 1.9422 - val_mse: 7.6544\n",
      "Epoch 461/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5688 - mae: 1.5863 - mse: 4.5688 - val_loss: 7.7463 - val_mae: 1.9533 - val_mse: 7.7463\n",
      "Epoch 462/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5278 - mae: 1.5924 - mse: 4.5278 - val_loss: 7.7888 - val_mae: 1.9466 - val_mse: 7.7888\n",
      "Epoch 463/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.4729 - mae: 1.5751 - mse: 4.4729 - val_loss: 7.7018 - val_mae: 1.9649 - val_mse: 7.7018\n",
      "Epoch 464/500\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 4.5241 - mae: 1.5839 - mse: 4.5241 - val_loss: 7.6673 - val_mae: 1.9478 - val_mse: 7.6673\n",
      "Epoch 465/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5081 - mae: 1.5840 - mse: 4.5081 - val_loss: 7.6427 - val_mae: 1.9367 - val_mse: 7.6427\n",
      "Epoch 466/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.4671 - mae: 1.5659 - mse: 4.4671 - val_loss: 7.7069 - val_mae: 1.9546 - val_mse: 7.7069\n",
      "Epoch 467/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5875 - mae: 1.6084 - mse: 4.5875 - val_loss: 7.7028 - val_mae: 1.9480 - val_mse: 7.7028\n",
      "Epoch 468/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6104 - mae: 1.5967 - mse: 4.6104 - val_loss: 7.7635 - val_mae: 1.9644 - val_mse: 7.7635\n",
      "Epoch 469/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5385 - mae: 1.5855 - mse: 4.5385 - val_loss: 7.7818 - val_mae: 1.9567 - val_mse: 7.7818\n",
      "Epoch 470/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.6624 - mae: 1.6019 - mse: 4.6624 - val_loss: 7.7532 - val_mae: 1.9778 - val_mse: 7.7532\n",
      "Epoch 471/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5725 - mae: 1.6125 - mse: 4.5725 - val_loss: 7.7101 - val_mae: 1.9844 - val_mse: 7.7101\n",
      "Epoch 472/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5036 - mae: 1.5873 - mse: 4.5036 - val_loss: 7.6187 - val_mae: 1.9275 - val_mse: 7.6187\n",
      "Epoch 473/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.4487 - mae: 1.5706 - mse: 4.4487 - val_loss: 7.7123 - val_mae: 1.9627 - val_mse: 7.7123\n",
      "Epoch 474/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.4463 - mae: 1.5734 - mse: 4.4463 - val_loss: 7.7854 - val_mae: 1.9660 - val_mse: 7.7854\n",
      "Epoch 475/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.4879 - mae: 1.5777 - mse: 4.4879 - val_loss: 7.6660 - val_mae: 1.9444 - val_mse: 7.6660\n",
      "Epoch 476/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.4154 - mae: 1.5570 - mse: 4.4154 - val_loss: 7.6637 - val_mae: 1.9614 - val_mse: 7.6637\n",
      "Epoch 477/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3841 - mae: 1.5645 - mse: 4.3841 - val_loss: 7.7619 - val_mae: 1.9578 - val_mse: 7.7619\n",
      "Epoch 478/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.4138 - mae: 1.5688 - mse: 4.4138 - val_loss: 7.6309 - val_mae: 1.9314 - val_mse: 7.6309\n",
      "Epoch 479/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.4014 - mae: 1.5634 - mse: 4.4014 - val_loss: 7.6347 - val_mae: 1.9501 - val_mse: 7.6347\n",
      "Epoch 480/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.4522 - mae: 1.5752 - mse: 4.4522 - val_loss: 7.6111 - val_mae: 1.9333 - val_mse: 7.6111\n",
      "Epoch 481/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3543 - mae: 1.5546 - mse: 4.3543 - val_loss: 7.6131 - val_mae: 1.9455 - val_mse: 7.6131\n",
      "Epoch 482/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.4529 - mae: 1.5734 - mse: 4.4529 - val_loss: 7.6404 - val_mae: 1.9565 - val_mse: 7.6404\n",
      "Epoch 483/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3906 - mae: 1.5516 - mse: 4.3906 - val_loss: 7.6755 - val_mae: 1.9429 - val_mse: 7.6755\n",
      "Epoch 484/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3600 - mae: 1.5480 - mse: 4.3600 - val_loss: 7.6126 - val_mae: 1.9379 - val_mse: 7.6126\n",
      "Epoch 485/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3378 - mae: 1.5483 - mse: 4.3378 - val_loss: 7.7150 - val_mae: 1.9638 - val_mse: 7.7150\n",
      "Epoch 486/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.4610 - mae: 1.5776 - mse: 4.4610 - val_loss: 7.5509 - val_mae: 1.9328 - val_mse: 7.5509\n",
      "Epoch 487/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3796 - mae: 1.5637 - mse: 4.3796 - val_loss: 7.6070 - val_mae: 1.9440 - val_mse: 7.6070\n",
      "Epoch 488/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5121 - mae: 1.5747 - mse: 4.5121 - val_loss: 7.6082 - val_mae: 1.9542 - val_mse: 7.6082\n",
      "Epoch 489/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3807 - mae: 1.5610 - mse: 4.3807 - val_loss: 7.6377 - val_mae: 1.9480 - val_mse: 7.6377\n",
      "Epoch 490/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3413 - mae: 1.5508 - mse: 4.3413 - val_loss: 7.5961 - val_mae: 1.9439 - val_mse: 7.5961\n",
      "Epoch 491/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3280 - mae: 1.5487 - mse: 4.3280 - val_loss: 7.5661 - val_mae: 1.9304 - val_mse: 7.5661\n",
      "Epoch 492/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3066 - mae: 1.5438 - mse: 4.3066 - val_loss: 7.6639 - val_mae: 1.9439 - val_mse: 7.6639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 493/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3375 - mae: 1.5566 - mse: 4.3375 - val_loss: 7.5782 - val_mae: 1.9376 - val_mse: 7.5782\n",
      "Epoch 494/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3133 - mae: 1.5459 - mse: 4.3133 - val_loss: 7.5654 - val_mae: 1.9385 - val_mse: 7.5654\n",
      "Epoch 495/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.2687 - mae: 1.5352 - mse: 4.2687 - val_loss: 7.6821 - val_mae: 1.9570 - val_mse: 7.6821\n",
      "Epoch 496/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3390 - mae: 1.5527 - mse: 4.3390 - val_loss: 7.5079 - val_mae: 1.9303 - val_mse: 7.5079\n",
      "Epoch 497/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3151 - mae: 1.5439 - mse: 4.3151 - val_loss: 7.5472 - val_mae: 1.9322 - val_mse: 7.5472\n",
      "Epoch 498/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3135 - mae: 1.5433 - mse: 4.3135 - val_loss: 7.5791 - val_mae: 1.9379 - val_mse: 7.5791\n",
      "Epoch 499/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.3221 - mae: 1.5576 - mse: 4.3221 - val_loss: 7.5592 - val_mae: 1.9350 - val_mse: 7.5592\n",
      "Epoch 500/500\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.2942 - mae: 1.5349 - mse: 4.2942 - val_loss: 7.6278 - val_mae: 1.9422 - val_mse: 7.6278\n"
     ]
    }
   ],
   "source": [
    "training_history = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            validation_split = 0.2,\n",
    "                            epochs=500,\n",
    "                            batch_size=100,\n",
    "                            callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed00825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir seq_logs --port 6060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27d5305",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e219a6b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[1;32m      2\u001b[0m r2_score(y_test, y_pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c98960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_sgd():\n",
    "    \n",
    "    model = keras.Sequential([layers.Dense(32, input_shape = (x_train.shape[1],), activation='relu'),\n",
    "                              layers.Dense(16, activation = 'relu'),\n",
    "                              layers.Dense(4, activation = 'relu'),\n",
    "                              layers.Dense(1)])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate = 0.001) #SGD instead of Adam\n",
    "    \n",
    "    model.compile(loss = 'mse', metrics = ['mae', 'mse'], optimizer = optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c4e0f42-7658-41b7-b37c-6b9e60a455e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_model_with_sgd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_sgd \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model_with_sgd\u001b[49m()\n\u001b[1;32m      2\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mplot_model(model_sgd, show_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_model_with_sgd' is not defined"
     ]
    }
   ],
   "source": [
    "model_sgd = build_model_with_sgd()\n",
    "tf.keras.utils.plot_model(model_sgd, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8962d8-24fe-4957-8344-179dab1bfbb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_sgd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_sgd\u001b[49m\u001b[38;5;241m.\u001b[39mfit(x_train,\n\u001b[1;32m      2\u001b[0m                             y_train,\n\u001b[1;32m      3\u001b[0m                             validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m      4\u001b[0m                             epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      5\u001b[0m                             batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_sgd' is not defined"
     ]
    }
   ],
   "source": [
    "training_history = model_sgd.fit(x_train,\n",
    "                            y_train,\n",
    "                            validation_split = 0.2,\n",
    "                            epochs=100,\n",
    "                            batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27bb7966-bb03-4d8a-be29-366301328907",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_sgd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_sgd\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_sgd' is not defined"
     ]
    }
   ],
   "source": [
    "model_sgd.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ed06a90-ec5c-4c32-8754-4acc5f9df731",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_sgd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_sgd\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[1;32m      2\u001b[0m r2_score(y_test, y_pred) \u001b[38;5;66;03m#r2 lower because of less epochs and different optimizer\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_sgd' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = model_sgd.predict(x_test)\n",
    "r2_score(y_test, y_pred) #r2 lower because of less epochs and different optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10633242-4d25-4162-8690-b181d8f70520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_rmsprop():\n",
    "    \n",
    "    model = keras.Sequential([layers.Dense(16, input_shape = (x_train.shape[1],), activation='elu'),\n",
    "                              layers.Dense(8, activation = 'elu'),\n",
    "                              layers.Dense(4, activation = 'elu'),\n",
    "                              layers.Dense(1)]) #elu similar to relu, but mitigates issue of saturated neurons\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.001) #SGD instead of Adam\n",
    "    \n",
    "    model.compile(loss = 'mse', metrics = ['mae', 'mse'], optimizer = optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15ee899f-c700-4718-9b8d-b4363e70e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rmsprop = build_model_with_rmsprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac2d7384-22ff-434a-a688-1a5370428337",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_rmsprop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                            \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "training_history = model_rmsprop.fit(x_train,\n",
    "                            y_train,\n",
    "                            validation_split = 0.2,\n",
    "                            epochs=100,\n",
    "                            batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0ecbf59-e180-40c1-9d7d-c9ad77939202",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_rmsprop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "model_rmsprop.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4481262-4765-4b46-b0e8-aeb6a98357e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_rmsprop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m r2_score(y_test, y_pred) \u001b[38;5;66;03m# higher r2 with just 100 epochs.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "y_pred = model_rmsprop.predict(x_test)\n",
    "r2_score(y_test, y_pred) # higher r2 with just 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "857deb32-f52e-49a0-aaf9-07086673c94b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e8263f0-f711-451e-a5d3-0fb45080e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4a6d27-7480-423a-b311-e87b210a40b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
